{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorization Machines\n",
    "Factorization Machines is a generalization of linear models.\n",
    "https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n",
    "\n",
    "They're well-suited for high\n",
    "dimension sparse datasets, such as user-item interaction matrices for recommendation.\n",
    "\n",
    "In this example, we're going to train a recommendation model based on the MovieLens\n",
    "dataset ( https://grouplens.org/datasets/movielens/ ).\n",
    "\n",
    "Factorization Machines is a supervised learning algorithm, so we need to train it on labeled samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a plain matrix, we'll use a sparse matrix, a data structure specifically\n",
    "designed and optimized for sparse datasets. Scipy has exactly the object we need,\n",
    "named lil_matrix ( https://docs.scipy.org/doc/scipy/reference/\n",
    "generated/scipy.sparse.lil_matrix.html ). This will help us to get rid\n",
    "of all these nasty zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding protobuf and RecordIO\n",
    "\n",
    "So how will we pass this sparse matrix to the SageMaker algorithm? As you would expect,\n",
    "we're going to serialize the object, and store it in S3. We're not going to use Python\n",
    "serialization, however. Instead, we're going to use protobuf ( https://developers.\n",
    "google.com/protocol-buffers/ ), a popular and efficient serialization mechanism.\n",
    "In addition, we're going to store the protobuf-encoded data in a record format called\n",
    "RecordIO ( https://mxnet.apache.org/api/faq/recordio/ ). Our dataset\n",
    "will be stored as a sequence of records in a single file. This has the following benefits:\n",
    "\n",
    "• A single file is easier to move around: who wants to deal with thousands\n",
    "of individual files that can get lost or corrupted?\n",
    "\n",
    "• A sequential file is faster to read, which makes the training process more efficient.\n",
    "\n",
    "• A sequence of records is easy to split for distributed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Factorization Machines model on MovieLens\n",
    "Download ml-100k dataset and extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2021-05-02 02:42:42--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  107K 44s\n",
      "    50K .......... .......... .......... .......... ..........  2%  199K 34s\n",
      "   100K .......... .......... .......... .......... ..........  3% 7.98M 22s\n",
      "   150K .......... .......... .......... .......... ..........  4% 6.27M 17s\n",
      "   200K .......... .......... .......... .......... ..........  5%  226K 17s\n",
      "   250K .......... .......... .......... .......... ..........  6% 6.98M 14s\n",
      "   300K .......... .......... .......... .......... ..........  7%  226K 15s\n",
      "   350K .......... .......... .......... .......... ..........  8% 8.13M 13s\n",
      "   400K .......... .......... .......... .......... ..........  9% 5.84M 12s\n",
      "   450K .......... .......... .......... .......... .......... 10%  228K 12s\n",
      "   500K .......... .......... .......... .......... .......... 11% 5.61M 11s\n",
      "   550K .......... .......... .......... .......... .......... 12% 7.03M 10s\n",
      "   600K .......... .......... .......... .......... .......... 13%  232K 11s\n",
      "   650K .......... .......... .......... .......... .......... 14% 4.78M 10s\n",
      "   700K .......... .......... .......... .......... .......... 15% 5.32M 9s\n",
      "   750K .......... .......... .......... .......... .......... 16%  227K 9s\n",
      "   800K .......... .......... .......... .......... .......... 17% 5.04M 9s\n",
      "   850K .......... .......... .......... .......... .......... 18% 8.38M 8s\n",
      "   900K .......... .......... .......... .......... .......... 19%  233K 9s\n",
      "   950K .......... .......... .......... .......... .......... 20% 8.71M 8s\n",
      "  1000K .......... .......... .......... .......... .......... 21% 5.40M 8s\n",
      "  1050K .......... .......... .......... .......... .......... 22%  232K 8s\n",
      "  1100K .......... .......... .......... .......... .......... 23% 11.4M 7s\n",
      "  1150K .......... .......... .......... .......... .......... 24% 5.12M 7s\n",
      "  1200K .......... .......... .......... .......... .......... 25%  237K 7s\n",
      "  1250K .......... .......... .......... .......... .......... 27% 4.95M 7s\n",
      "  1300K .......... .......... .......... .......... .......... 28% 5.80M 7s\n",
      "  1350K .......... .......... .......... .......... .......... 29% 4.43M 6s\n",
      "  1400K .......... .......... .......... .......... .......... 30%  220K 7s\n",
      "  1450K .......... .......... .......... .......... .......... 31% 6.36M 6s\n",
      "  1500K .......... .......... .......... .......... .......... 32% 3.84M 6s\n",
      "  1550K .......... .......... .......... .......... .......... 33% 6.18M 6s\n",
      "  1600K .......... .......... .......... .......... .......... 34%  243K 6s\n",
      "  1650K .......... .......... .......... .......... .......... 35% 4.17M 6s\n",
      "  1700K .......... .......... .......... .......... .......... 36% 7.06M 5s\n",
      "  1750K .......... .......... .......... .......... .......... 37% 6.48M 5s\n",
      "  1800K .......... .......... .......... .......... .......... 38%  243K 5s\n",
      "  1850K .......... .......... .......... .......... .......... 39% 4.03M 5s\n",
      "  1900K .......... .......... .......... .......... .......... 40% 9.88M 5s\n",
      "  1950K .......... .......... .......... .......... .......... 41% 5.51M 5s\n",
      "  2000K .......... .......... .......... .......... .......... 42%  243K 5s\n",
      "  2050K .......... .......... .......... .......... .......... 43% 4.66M 5s\n",
      "  2100K .......... .......... .......... .......... .......... 44% 6.79M 4s\n",
      "  2150K .......... .......... .......... .......... .......... 45% 8.06M 4s\n",
      "  2200K .......... .......... .......... .......... .......... 46%  238K 4s\n",
      "  2250K .......... .......... .......... .......... .......... 47% 4.36M 4s\n",
      "  2300K .......... .......... .......... .......... .......... 48% 5.58M 4s\n",
      "  2350K .......... .......... .......... .......... .......... 49% 6.94M 4s\n",
      "  2400K .......... .......... .......... .......... .......... 50%  265K 4s\n",
      "  2450K .......... .......... .......... .......... .......... 51% 2.04M 4s\n",
      "  2500K .......... .......... .......... .......... .......... 53% 8.04M 4s\n",
      "  2550K .......... .......... .......... .......... .......... 54% 5.17M 3s\n",
      "  2600K .......... .......... .......... .......... .......... 55% 6.68M 3s\n",
      "  2650K .......... .......... .......... .......... .......... 56%  252K 3s\n",
      "  2700K .......... .......... .......... .......... .......... 57% 4.24M 3s\n",
      "  2750K .......... .......... .......... .......... .......... 58% 5.98M 3s\n",
      "  2800K .......... .......... .......... .......... .......... 59% 5.09M 3s\n",
      "  2850K .......... .......... .......... .......... .......... 60% 4.57M 3s\n",
      "  2900K .......... .......... .......... .......... .......... 61%  258K 3s\n",
      "  2950K .......... .......... .......... .......... .......... 62% 5.01M 3s\n",
      "  3000K .......... .......... .......... .......... .......... 63% 6.04M 3s\n",
      "  3050K .......... .......... .......... .......... .......... 64% 5.86M 2s\n",
      "  3100K .......... .......... .......... .......... .......... 65% 4.94M 2s\n",
      "  3150K .......... .......... .......... .......... .......... 66%  257K 2s\n",
      "  3200K .......... .......... .......... .......... .......... 67% 5.55M 2s\n",
      "  3250K .......... .......... .......... .......... .......... 68% 5.77M 2s\n",
      "  3300K .......... .......... .......... .......... .......... 69% 6.85M 2s\n",
      "  3350K .......... .......... .......... .......... .......... 70% 4.89M 2s\n",
      "  3400K .......... .......... .......... .......... .......... 71%  258K 2s\n",
      "  3450K .......... .......... .......... .......... .......... 72% 5.01M 2s\n",
      "  3500K .......... .......... .......... .......... .......... 73% 2.20M 2s\n",
      "  3550K .......... .......... .......... .......... .......... 74% 6.64M 2s\n",
      "  3600K .......... .......... .......... .......... .......... 75% 3.46M 2s\n",
      "  3650K .......... .......... .......... .......... .......... 76% 9.56M 1s\n",
      "  3700K .......... .......... .......... .......... .......... 77%  284K 1s\n",
      "  3750K .......... .......... .......... .......... .......... 79% 5.12M 1s\n",
      "  3800K .......... .......... .......... .......... .......... 80% 2.24M 1s\n",
      "  3850K .......... .......... .......... .......... .......... 81% 9.32M 1s\n",
      "  3900K .......... .......... .......... .......... .......... 82% 3.04M 1s\n",
      "  3950K .......... .......... .......... .......... .......... 83%  316K 1s\n",
      "  4000K .......... .......... .......... .......... .......... 84% 2.07M 1s\n",
      "  4050K .......... .......... .......... .......... .......... 85% 5.52M 1s\n",
      "  4100K .......... .......... .......... .......... .......... 86% 2.52M 1s\n",
      "  4150K .......... .......... .......... .......... .......... 87% 8.68M 1s\n",
      "  4200K .......... .......... .......... .......... .......... 88% 3.21M 1s\n",
      "  4250K .......... .......... .......... .......... .......... 89%  317K 1s\n",
      "  4300K .......... .......... .......... .......... .......... 90% 2.30M 1s\n",
      "  4350K .......... .......... .......... .......... .......... 91% 4.70M 1s\n",
      "  4400K .......... .......... .......... .......... .......... 92% 2.44M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 93% 9.93M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 94% 2.77M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 95% 10.9M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 96%  287K 0s\n",
      "  4650K .......... .......... .......... .......... .......... 97% 5.78M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 98% 3.62M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 99% 4.86M 0s\n",
      "  4800K ........                                              100% 11.9M=5.9s\n",
      "\n",
      "2021-05-02 02:42:48 (819 KB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\t946\t3\t880088363\r\n",
      "453\t238\t4\t877554396\r\n",
      "58\t1106\t4\t892068866\r\n",
      "350\t181\t4\t882346720\r\n",
      "617\t174\t1\t883788820\r\n"
     ]
    }
   ],
   "source": [
    "# Going in folder\n",
    "%cd ml-100k\n",
    "!shuf ua.base -o ua.base.shuffled  # Shuffling it\n",
    "!head -5 ua.base.shuffled          # Printing 5 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users=943\n",
    "num_movies=1682\n",
    "num_features=num_users+num_movies\n",
    "\n",
    "num_ratings_train=90570\n",
    "num_ratings_test=9430"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a function to load a dataset into a sparse matrix. Based on the\n",
    "previous explanation, we go through the dataset line by line. In the X matrix,\n",
    "we set the appropriate user and movie columns to 1 . We also store the rating in the\n",
    "Y vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def loadDataset(filename, lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter='\\t')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            X[line,int(num_users)+int(movieId)-1] = 1\n",
    "            Y.append(int(rating))\n",
    "            line=line+1       \n",
    "    Y=np.array(Y).astype('float32')\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = loadDataset('ua.base.shuffled', num_ratings_train, num_features)\n",
    "X_test, Y_test = loadDataset('ua.test', num_ratings_test, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 2625)\n",
      "(90570,)\n",
      "(9430, 2625)\n",
      "(9430,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (num_ratings_train, num_features)\n",
    "assert Y_train.shape == (num_ratings_train, )\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (num_ratings_test, num_features)\n",
    "assert Y_test.shape  == (num_ratings_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 2625)\n",
      "(90570,)\n",
      "(9430, 2625)\n",
      "(9430,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to protobuf and save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'fm-movielens'\n",
    "\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a function that converts a dataset to the RecordIO-wrapped\n",
    "protobuf , and uploads it to an S3 bucket. We first create an in-memory binary\n",
    "stream with io.BytesIO() . Then, we use the life-saving write_spmatrix_\n",
    "to_sparse_tensor( ) function to write the sample matrix and the label vector to\n",
    "that buffer in protobuf format. Finally, we use boto3 to upload the buffer to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BytesIO object at 0x7ffabb230270>\n",
      "<_io.BytesIO object at 0x7ffabb230270>\n",
      "s3://sagemaker-us-east-1-603012210694/fm-movielens/train/train.protobuf\n",
      "s3://sagemaker-us-east-1-603012210694/fm-movielens/test/test.protobuf\n",
      "Output: s3://sagemaker-us-east-1-603012210694/fm-movielens/output\n"
     ]
    }
   ],
   "source": [
    "import io, boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    # use smac.write_numpy_to_dense_tensor(buf, feature, label) for numpy arrays\n",
    "    buf.seek(0)\n",
    "    print(buf)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key)    \n",
    "test_data  = writeDatasetToProtobuf(X_test, Y_test, bucket, test_prefix, test_key)    \n",
    "  \n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::603012210694:role/service-role/AmazonSageMaker-ExecutionRole-20210304T123661\n"
     ]
    }
   ],
   "source": [
    "# Extra step for local user only\n",
    "\n",
    "import boto3\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "def resolve_sm_role():\n",
    "    client = boto3.client('iam', region_name=region)\n",
    "    response_roles = client.list_roles(\n",
    "        PathPrefix='/',\n",
    "        # Marker='string',\n",
    "        MaxItems=999\n",
    "    )\n",
    "    for role in response_roles['Roles']:\n",
    "        if role['RoleName'].startswith('AmazonSageMaker-ExecutionRole-'):\n",
    "            #print('Resolved SageMaker IAM Role to: ' + str(role))\n",
    "            return role['Arn']\n",
    "    raise Exception('Could not resolve what should be the SageMaker role to be used')\n",
    "\n",
    "role = resolve_sm_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training job\n",
    "We find the name of the Factorization Machines container, configure the Estimator function, and set the\n",
    "hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "container = image_uris.retrieve('factorization-machines', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = sagemaker.estimator.Estimator(container,\n",
    "                                   role=role,#sagemaker.get_execution_role(),\n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.c5.xlarge',\n",
    "                                   output_path=output_prefix\n",
    "                                   )\n",
    "\n",
    "fm.set_hyperparameters(feature_dim=num_features,\n",
    "                      predictor_type='regressor',\n",
    "                      num_factors=64,\n",
    "                      epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then launch the training job. Did you notice that we didn't configure training\n",
    "inputs? We're simply passing the location of the two protobuf files. As protobuf\n",
    "is the default format for Factorization Machines (as well as other built-in\n",
    "algorithms), we can save a step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-01 22:22:53 Starting - Starting the training job...\n",
      "2021-05-01 22:22:55 Starting - Launching requested ML instancesProfilerReport-1619907772: InProgress\n",
      "......\n",
      "2021-05-01 22:24:23 Starting - Preparing the instances for training...\n",
      "2021-05-01 22:25:04 Downloading - Downloading input data...\n",
      "2021-05-01 22:25:43 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:37 INFO 140392802047808] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:37 INFO 140392802047808] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '2625', 'predictor_type': 'regressor', 'num_factors': '64', 'epochs': '10'}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:37 INFO 140392802047808] Final configuration: {'epochs': '10', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '2625', 'predictor_type': 'regressor', 'num_factors': '64'}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:37 WARNING 140392802047808] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:37 INFO 140392802047808] Using default worker.\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:37 INFO 140392802047808] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:37.978] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:37.983] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 8, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907937.9749722, \"EndTime\": 1619907938.0185814, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 37.555694580078125, \"count\": 1, \"min\": 37.555694580078125, \"max\": 37.555694580078125}}}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907938.0186987, \"EndTime\": 1619907938.0187316, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[22:25:38] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204642.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[22:25:38] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.204642.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=3.747651738714191\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=14.0448935546875\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=3.579564208984375\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:38.604] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 565, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.7504101476632545\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=0, train mse <loss>=3.0639356850424964\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=1.3890064797873025\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907938.0186503, \"EndTime\": 1619907938.6045825, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"update.time\": {\"sum\": 585.6218338012695, \"count\": 1, \"min\": 585.6218338012695, \"max\": 585.6218338012695}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907938.0189333, \"EndTime\": 1619907938.6047914, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 91570.0, \"count\": 1, \"min\": 91570, \"max\": 91570}, \"Total Batches Seen\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=154568.3345248408 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.1053688669540362\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.22184033203125\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:38 INFO 140392802047808] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.9283093872070313\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:39.104] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 497, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.131556371821144\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.2804198226090315\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.9467589460100446\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907938.6046555, \"EndTime\": 1619907939.104906, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 499.603271484375, \"count\": 1, \"min\": 499.603271484375, \"max\": 499.603271484375}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907938.6052768, \"EndTime\": 1619907939.1051273, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 182140.0, \"count\": 1, \"min\": 182140, \"max\": 182140}, \"Total Batches Seen\": {\"sum\": 183.0, \"count\": 1, \"min\": 183, \"max\": 183}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=181155.3759274078 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.0898442540321416\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.187760498046875\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.9150439453125\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:39.745] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 638, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.113699428039552\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.240326416015625\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.9299126982636504\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907939.1049743, \"EndTime\": 1619907939.746847, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 641.1843299865723, \"count\": 1, \"min\": 641.1843299865723, \"max\": 641.1843299865723}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907939.105635, \"EndTime\": 1619907939.7473767, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 272710.0, \"count\": 1, \"min\": 272710, \"max\": 272710}, \"Total Batches Seen\": {\"sum\": 274.0, \"count\": 1, \"min\": 274, \"max\": 274}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=141109.46634963807 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=1.0712966506397166\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=1.147676513671875\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:39 INFO 140392802047808] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.89725146484375\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:40.370] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 621, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:40 INFO 140392802047808] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.0947011164572975\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:40 INFO 140392802047808] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.1983705343728537\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:40 INFO 140392802047808] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.9110406534383585\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907939.7470925, \"EndTime\": 1619907940.3713546, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 623.4338283538818, \"count\": 1, \"min\": 623.4338283538818, \"max\": 623.4338283538818}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:40 INFO 140392802047808] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907939.7478964, \"EndTime\": 1619907940.371618, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 363280.0, \"count\": 1, \"min\": 363280, \"max\": 363280}, \"Total Batches Seen\": {\"sum\": 365.0, \"count\": 1, \"min\": 365, \"max\": 365}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:40 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=145181.54596871557 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:40 INFO 140392802047808] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=1.0521811655214728\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:40 INFO 140392802047808] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=1.107085205078125\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:40 INFO 140392802047808] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.8778296508789063\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:41.002] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 628, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=1.0756519952905435\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=4, train mse <loss>=1.1570272149725274\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.8909976464575463\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907940.3714228, \"EndTime\": 1619907941.0030317, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 630.8174133300781, \"count\": 1, \"min\": 630.8174133300781, \"max\": 630.8174133300781}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907940.3721552, \"EndTime\": 1619907941.0032406, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 453850.0, \"count\": 1, \"min\": 453850, \"max\": 453850}, \"Total Batches Seen\": {\"sum\": 456.0, \"count\": 1, \"min\": 456, \"max\": 456}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=143489.38906033398 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=1.0338693651800563\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=1.0688858642578125\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.8580176391601563\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:41.627] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 622, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=1.0575872983772219\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=5, train mse <loss>=1.1184908936888307\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.8708440303592891\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907941.0030925, \"EndTime\": 1619907941.6283286, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 624.5338916778564, \"count\": 1, \"min\": 624.5338916778564, \"max\": 624.5338916778564}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907941.0037637, \"EndTime\": 1619907941.6285577, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 544420.0, \"count\": 1, \"min\": 544420, \"max\": 544420}, \"Total Batches Seen\": {\"sum\": 547.0, \"count\": 1, \"min\": 547, \"max\": 547}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=144934.22578383068 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=1.0170995905752973\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=1.0344915771484375\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:41 INFO 140392802047808] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=0.8386442260742187\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:42.156] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 525, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=1.0410830759555962\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=6, train mse <loss>=1.0838539710411659\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.8515455925910027\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907941.6283953, \"EndTime\": 1619907942.1565518, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 527.3985862731934, \"count\": 1, \"min\": 527.3985862731934, \"max\": 527.3985862731934}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907941.6291282, \"EndTime\": 1619907942.1567075, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 634990.0, \"count\": 1, \"min\": 634990, \"max\": 634990}, \"Total Batches Seen\": {\"sum\": 638.0, \"count\": 1, \"min\": 638, \"max\": 638}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=171642.48289124225 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=1.0021669332060994\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=1.0043385620117187\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.8206585083007812\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2021-05-01 22:25:42.698] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 539, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=1.0263907882225434\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=7, train mse <loss>=1.053478050148094\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.8341658989204155\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907942.156609, \"EndTime\": 1619907942.6989508, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 541.7196750640869, \"count\": 1, \"min\": 541.7196750640869, \"max\": 541.7196750640869}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907942.1572042, \"EndTime\": 1619907942.699156, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 725560.0, \"count\": 1, \"min\": 725560, \"max\": 725560}, \"Total Batches Seen\": {\"sum\": 729.0, \"count\": 1, \"min\": 729, \"max\": 729}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=167084.4884320643 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=0.989106436622849\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=0.97833154296875\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:42 INFO 140392802047808] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.804816650390625\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:43.231] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 530, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=1.0135486207660303\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=8, train mse <loss>=1.0272808066567223\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.8193050986489097\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907942.6990113, \"EndTime\": 1619907943.2319515, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 532.2287082672119, \"count\": 1, \"min\": 532.2287082672119, \"max\": 532.2287082672119}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907942.6996984, \"EndTime\": 1619907943.232099, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 816130.0, \"count\": 1, \"min\": 816130, \"max\": 816130}, \"Total Batches Seen\": {\"sum\": 820.0, \"count\": 1, \"min\": 820, \"max\": 820}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=170089.21950116527 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=0.9778160392133936\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=0.9561242065429687\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.7916079711914062\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:43.781] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 548, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=1.002460970460034\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=9, train mse <loss>=1.004927997295673\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.80697461608216\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, train rmse <loss>=1.002460970460034\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, train mse <loss>=1.004927997295673\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, train absolute_loss <loss>=0.80697461608216\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907943.2320058, \"EndTime\": 1619907943.7829258, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 550.2865314483643, \"count\": 1, \"min\": 550.2865314483643, \"max\": 550.2865314483643}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907943.2326112, \"EndTime\": 1619907943.7831526, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 906700.0, \"count\": 1, \"min\": 906700, \"max\": 906700}, \"Total Batches Seen\": {\"sum\": 911.0, \"count\": 1, \"min\": 911, \"max\": 911}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #throughput_metric: host=algo-1, train throughput=164476.10035395357 records/second\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 WARNING 140392802047808] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907943.7829947, \"EndTime\": 1619907943.7855744, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 2.1207332611083984, \"count\": 1, \"min\": 2.1207332611083984, \"max\": 2.1207332611083984}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] Saved checkpoint to \"/tmp/tmp1uolyiv4/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:43.792] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 5813, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[34m[2021-05-01 22:25:43.824] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 31, \"num_examples\": 10, \"num_bytes\": 603520}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907943.7925282, \"EndTime\": 1619907943.8244162, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #test_score (algo-1) : ('rmse', 1.0203627755246003)\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #test_score (algo-1) : ('mse', 1.0411401936762659)\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #test_score (algo-1) : ('absolute_loss', 0.8393011681727615)\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, test rmse <loss>=1.0203627755246003\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, test mse <loss>=1.0411401936762659\u001b[0m\n",
      "\u001b[34m[05/01/2021 22:25:43 INFO 140392802047808] #quality_metric: host=algo-1, test absolute_loss <loss>=0.8393011681727615\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1619907943.785648, \"EndTime\": 1619907943.825565, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 23.456096649169922, \"count\": 1, \"min\": 23.456096649169922, \"max\": 23.456096649169922}, \"totaltime\": {\"sum\": 5879.355192184448, \"count\": 1, \"min\": 5879.355192184448, \"max\": 5879.355192184448}}}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-05-01 22:26:03 Uploading - Uploading generated training model\n",
      "2021-05-01 22:26:03 Completed - Training job completed\n",
      "Training seconds: 51\n",
      "Billable seconds: 51\n"
     ]
    }
   ],
   "source": [
    "fm.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'fm-movielens-100k'\n",
    "fm_predictor = fm.deploy(endpoint_name=endpoint_name,\n",
    "                         instance_type='ml.t2.medium', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now send samples to the endpoint in JSON format ( https://docs.aws.\n",
    "amazon.com/sagemaker/latest/dg/fact-machines.html#fm-\n",
    "inputoutput ). For this purpose, we write a custom serializer to convert input\n",
    "data to JSON. The default JSON deserializer will be used automatically since we set\n",
    "the content type to 'application/json' :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "       js = {'instances': []}\n",
    "       for row in data:\n",
    "              js['instances'].append({'features': row.tolist()})\n",
    "       return json.dumps(js)\n",
    "\n",
    "fm_predictor.serializer = FMSerializer()\n",
    "fm_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'score': 3.3867130279541016}, {'score': 3.422882556915283}, {'score': 3.622199535369873}]}\n"
     ]
    }
   ],
   "source": [
    "result = fm_predictor.predict(X_test[:3].toarray())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAGEMAKER",
   "language": "python",
   "name": "sagemaker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
